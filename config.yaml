server:
  host: "127.0.0.1"
  port: 3000
  timeout: 30

chatglm:
  api_url: "https://open.bigmodel.cn/api/paas/v4/chat/completions"
  api_key: "${CHATGLM_API_KEY}"
  model: "glm-4"
  max_tokens: 8192
  temperature: 0.7
  top_p: 0.7
  stream: true

cors:
  allowed_origins:
    - "http://localhost:3000"
    - "http://127.0.0.1:3000"
  allowed_methods:
    - "GET"
    - "POST"
    - "PUT"
    - "DELETE"
    - "OPTIONS"
  allowed_headers:
    - "Content-Type"
    - "Authorization"

logging:
  level: "info"
  format: "json"

static_files:
  path: "./static"
  max_age: 3600

session:
  secret: "${SESSION_SECRET}"
  timeout: 1800

websocket:
  max_connections: 100
  heartbeat_interval: 30
